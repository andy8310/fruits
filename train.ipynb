{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import keras as kr\n",
    "kr.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-0b23733f3e06>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-0b23733f3e06>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip3 install keras\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip3 install keras\n",
    "Using TensorFlow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-15-d7f7742b2ef9>, line 142)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-d7f7742b2ef9>\"\u001b[1;36m, line \u001b[1;32m142\u001b[0m\n\u001b[1;33m    #                     callbacks=[csv_logger, checkpointer])\u001b[0m\n\u001b[1;37m                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger\n",
    "import keras.backend as K\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(\"Loading metadata...\")\n",
    "\n",
    "class_to_ix = {}\n",
    "ix_to_class = {}\n",
    "with open('food/meta/classes.txt', 'r') as txt:\n",
    "    classes = [l.strip() for l in txt.readlines()]\n",
    "    class_to_ix = dict(zip(classes, range(len(classes))))\n",
    "    ix_to_class = dict(zip(range(len(classes)), \n",
    "    class_to_ix = {v: k for k, v in ix_to_class.items()}\n",
    "\n",
    "# # ####### Load concatenated data from disk\n",
    "# print(\n",
    "# h = h5py.File('X_all.hdf5'\n",
    "# X_all = np.array(h.get(\n",
    "# y_all = np.array(h.get(\n",
    "# h.close()\n",
    "\n",
    "# ####### Create train/val/test split\n",
    "# print(\"Creating train/val/test/split\")\n",
    "# n_classes = len(np.unique(y_all))\n",
    "\n",
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(X_all, y_all, test_size=.20, stratify=\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=.5, stratify=y_val_test)\n",
    "\n",
    "# y_train_cat = to_categorical(y_train, nb_classes=\n",
    "# y_val_cat = to_categorical(y_val, nb_classes=\n",
    "# y_test_cat = to_categorical(y_test, nb_classes=n_classes)\n",
    "\n",
    "# X_all = \n",
    "# X_val_test = \n",
    "# y_val_test = None\n",
    "\n",
    "# print(\n",
    "# h = h5py.File('X_test.hdf5', \n",
    "# h.create_dataset('data', data=X_test)\n",
    "# h.create_dataset('classes', data=y_test_cat)\n",
    "# h.close()\n",
    "\n",
    "                 \n",
    "                 \n",
    "\n",
    "\n",
    "# ######## Set up Image Augmentation\n",
    "# print(\"Setting up ImageDataGenerator\")\n",
    "# datagen = ImageDataGenerator(\n",
    "#     featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "#     samplewise_center=False,  \n",
    "#     featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "#     samplewise_std_normalization=False,  # divide each input by its std\n",
    "#     zca_whitening=False,  # apply ZCA whitening\n",
    "#     rotation_range=45,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "#     width_shift_range=0.125,  # randomly shift images horizontally (fraction of total width)\n",
    "#     height_shift_range=0.125,  \n",
    "#     horizontal_flip=True,  # randomly flip images\n",
    "#     vertical_flip=False, # randomly flip images\n",
    "#     rescale=1./255,\n",
    "#     fill_mode='nearest'\n",
    "# datagen.fit(X_train\n",
    "# generator = datagen.flow(X_train, y_train_cat, batch_size=32)\n",
    "# val_generator = datagen.flow(X_val, y_val_cat, batch_size=32)\n",
    "\n",
    "\n",
    "# ## Fine tuning. 70% with image augmentation.\n",
    "# ## 83% with pre processing (14 mins).\n",
    "# ## 84.5% with rmsprop/img.aug/dropout\n",
    "# ## 86.09% with batchnorm/dropout/img.aug/adam(10)/rmsprop(140)\n",
    "# ## InceptionV3\n",
    "\n",
    "# K.clear_session()\n",
    "\n",
    "# base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=Input(shape=(299, 299, 3\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x\n",
    "# # # x = Flatten()(x)\n",
    "# x = Dense(4096)(x\n",
    "# x = BatchNormalization()(x\n",
    "# x = Activation('relu')(x\n",
    "# x = Dropout(.5)(x)\n",
    "# predictions = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "# # x = base_model.output\n",
    "# # x = AveragePooling2D((8, 8), strides=(8, 8), name='avg_pool')(x)\n",
    "# # x = Flatten(name='flatten')(x)\n",
    "# # predictions = Dense(101, activation='softmax', name='predictions')(x)\n",
    "\n",
    "# model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# print(\"First pass\")\n",
    "# checkpointer = ModelCheckpoint(filepath='/home/stratospark/Code/AI/food101/first.3.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True\n",
    "# csv_logger = CSVLogger('first.3.log')\n",
    "# model.fit_generator(generator,\n",
    "#                     validation_data=val_generator,\n",
    "#                     nb_val_samples=10000,\n",
    "#                     samples_per_epoch=X_train.shape[0],\n",
    "#                     nb_epoch=10,\n",
    "#                     verbose=1,\n",
    "#                     callbacks=[csv_logger, checkpointer])\n",
    "\n",
    "# for layer in model.layers[:172]:\n",
    "#     layer.trainable = False\n",
    "# for layer in model.layers[172:]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# print(\"Second pass\")\n",
    "# model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'\n",
    "# checkpointer = ModelCheckpoint(filepath='/home/stratospark/Code/AI/food101/second.3.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True)\n",
    "# csv_logger = CSVLogger('second.3.log')\n",
    "# model.fit_generator(generator,\n",
    "#                     validation_data=val_generator,\n",
    "#                     nb_val_samples=10000,\n",
    "#                     samples_per_epoch=X_train.shape[0],\n",
    "#                     nb_epoch=100,\n",
    "#                     verbose=1,\n",
    "#                     callbacks=[csv_logger, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
